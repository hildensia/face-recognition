#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass scrartcl
\begin_preamble
\usepackage{todonotes}

\setkomafont{caption}{\sffamily}
\setkomafont{captionlabel}{\sffamily}
\setkomafont{sectioning}{\rmfamily\bfseries}

\clubpenalty=10000
\widowpenalty=10000
\end_preamble
\options a4wide, bibtotoc, chapterprefix,  numbers=noendperiod, abstracton, BCOR=5mm
\use_default_options true
\language ngerman
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Eigenfaces zur Gesichtserkennung
\end_layout

\begin_layout Author
Johannes Kulick
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Einleitung
\end_layout

\begin_layout Itemize
Gesichtserkennung als Sicherheitsfeature
\end_layout

\begin_layout Itemize
Multimediaanwendung
\end_layout

\begin_layout Itemize
Geschichtliches (Eigenfaces als erste relevante Technik)
\end_layout

\begin_layout Section
Gesichtserkennung als Mathematisches Problem
\end_layout

\begin_layout Standard
Gesichtserkennung wird in dieser Arbeit als Problem auf zwei dimensionalen
 Grauwertbildern aufgefasst.
 Die Eingabe für entsprechende Algorithmen ist also eine 
\begin_inset Formula $n\times m$
\end_inset

 Grauwertmatrix, das Bild.
 Jeder Grauwert ist 8 Bit groß, ist also ein Wert aus dem Intervall 
\begin_inset Formula $[0,255]$
\end_inset

.
\end_layout

\begin_layout Standard
Zusätzlich gibt es 
\begin_inset Formula $t$
\end_inset

 Trainingsbilder, denen jeweils ein Schlüssel  
\begin_inset Formula $k_{i}\in k_{1},\ldots,k_{j},j\le t$
\end_inset

 zugeordnet ist, die Person.
 Die Aufgabe des Gesichtserkennungsalgorithmus ist nun bei Eingabe eines
 neuen Bildes den Schlüssel 
\begin_inset Formula $k_{l}$
\end_inset


\end_layout

\begin_layout Itemize
2d bilder
\end_layout

\begin_layout Section
Eigenfaces als Repräsentation von Gesichtern
\end_layout

\begin_layout Subsection
PCA
\end_layout

\begin_layout Standard
Der Eigenfaces Algorithmus beruht im Wesentlichen auf einer Hauptkomponentenanal
yse der Trainingsbilder.
 Die Hauptkomponenten dienen im weiteren Verlauf als Fingerabdruck eines
 Gesichts.
\end_layout

\begin_layout Standard
Die Hauptkomponentenanalyse ist ein statistisches Verfahren, bei dem in
 einer 
\begin_inset Formula $n$
\end_inset

-dimensionale Punktmenge die Basis gefunden wird, bei der die Punkte in
 Richtung der Basisvektoren die größte Varianz aufweisen.
 Diese Basisvektoren sind die sogenannten Hauptkomponenten, sortiert nach
 der Varianz der Daten bezüglich dieses Basisvektors.
 Die erste Hauptkomponente, ist also genau der Basisvektor, in dessen Richtung
 die Punkte die größte Varianz aufweisen.
 Für eine Menge zwei dimensionaler Punkte ist dies in Abbildung 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Ein-Datencluster"

\end_inset

 dargestellt.
\end_layout

\begin_layout Standard
Das Problem der Findung der Hauptkomponenten wurde zuerst als geometrisches
 Problem durch Pearson beschrieben 
\begin_inset CommandInset citation
LatexCommand cite
key "pearson-1901"

\end_inset

.
 Der hier beschriebene Ansatz zur Berechnung der Haupotkomponenten als Optimieru
ngsproblem geht auf Hotelling 
\begin_inset CommandInset citation
LatexCommand cite
key "hotelling-1933"

\end_inset

 zurück und wird in vielen Textbüchern zu statistischen Verfahren beschrieben
 (zum Beispiel 
\begin_inset CommandInset citation
LatexCommand cite
key "webb-2002"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pca.png
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout

\family sans
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Ein-Datencluster"

\end_inset

Ein Datencluster im 2-dimensionalen Raum mit den beiden Hauptkomponenten
 (rot)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Seien 
\begin_inset Formula $X_{1},\ldots,X_{m}\in\mathbb{R}^{n}$
\end_inset

 Vektoren und 
\begin_inset Formula $M\in\mathbb{R}^{m\times n}$
\end_inset

 eine 
\begin_inset Formula $m\times n$
\end_inset

 Matrix, mit 
\begin_inset Formula $M=\left(\begin{array}{cccc}
X_{1} & X_{2} & \ldots & X_{m}\end{array}\right)$
\end_inset

.
 Ziel ist es nun, die Hauptkomponenten dieser Datenmatrix zu finden.
\end_layout

\begin_layout Standard
Wir fassen nun jeden Vektor 
\begin_inset Formula $X_{i}$
\end_inset

 als Belegung eines 
\begin_inset Formula $n$
\end_inset

-dimensionalen Zufallsvektors 
\begin_inset Formula $X$
\end_inset

 auf.
 Wir nehmen an, dass diese Belegungen normalverteilt um ihren Mittelwert
 
\begin_inset Formula $\mu=\frac{1}{m}\sum_{i=1}^{m}X_{i}$
\end_inset

 sind, also gilt auch das der Erwartungswert 
\begin_inset Formula $E[X]=\mu$
\end_inset

 ist und wir die Kovarianzmatrix
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\Sigma=E\left[(X-\mu)(X-\mu)^{T}\right]=E\left[XX^{T}\right]-\mu\mu^{T}\]

\end_inset

haben.
\end_layout

\begin_layout Standard
Obiges Problem ist nun analog dazu, neue Zufallsvariablen 
\begin_inset Formula $Z$
\end_inset

 zu finden, die eine Lineartransformation von 
\begin_inset Formula $X$
\end_inset

 sind, also 
\begin_inset Formula $Z_{i}=a_{i}^{T}X$
\end_inset

 mit 
\begin_inset Formula $a$
\end_inset

 einer noch zu findenden 
\begin_inset Formula $(n\times n)$
\end_inset

 Matrix von konstanten Koeffizienten.
 Diese neuen Variablen sollten möglichst unkorreliert sein, die Varianz
 der einzelnen Zufallsvariablen soll also möglichst hoch sein.
\end_layout

\begin_layout Standard
Um Abstände nach dem Basiswechsel gleich zu halten führen wir außerdem die
 Nebenbedingung ein, dass 
\begin_inset Formula $a_{j}^{T}a_{j}=\sum_{k=1}^{n}a_{jk}^{2}=1$
\end_inset

.
 Dies entspricht einer Normierung der Basisvektoren auf die gleiche Länge
 wie zuvor, die Abbildung wird also orthogonal.
\end_layout

\begin_layout Standard
Die erste Hauptkomponente ist nun also die Zufallsvariable 
\begin_inset Formula $Z_{1}$
\end_inset

, mit der höchsten Varainz.
 Es gilt
\end_layout

\begin_layout Standard
\begin_inset Formula \[
Var\left(Z_{i}\right)=Var\left(a_{i}^{T}X\right)=E\left[a_{i}^{T}(X-\mu)(X-\mu)^{T}a_{i}\right]=a_{i}^{T}\Sigma a_{i}.\]

\end_inset


\end_layout

\begin_layout Standard
Um die Hauptkomponenten zu finden müssen wir also die Zielfunktion 
\begin_inset Formula $a_{i}^{T}\Sigma a_{i}$
\end_inset

 maximieren, unter der gegebenen Nebenbedingung.
 Dies ist ein Optimierungsproblem.
 Zur Lösung nutzen wir Lagrange-Multiplikatoren 
\begin_inset CommandInset citation
LatexCommand cite
key "vapnyarskii-2001"

\end_inset

.
 Die Lagrange Funktion mit dem Lagrange-Multiplikator 
\begin_inset Formula $\lambda_{i}$
\end_inset

, die wir erhalten, ist
\begin_inset Newline newline
\end_inset


\begin_inset Formula \[
L(a_{i})=a_{i}^{T}\Sigma a_{i}-\lambda_{i}(a_{i}^{T}a_{i}-1).\]

\end_inset


\end_layout

\begin_layout Standard
Wir differenzieren nach 
\begin_inset Formula $a_{i}^{T}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\frac{\partial L}{\partial a_{i}^{T}}=2\Sigma a_{i}-2\lambda_{i}a_{i}\]

\end_inset


\end_layout

\begin_layout Standard
Um das Maximum zu erhalten setzen wir mit Null gleich und erhalten:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
0 & = & 2\Sigma a_{i}-2\lambda_{i}a_{i}\\
0 & = & (\Sigma-\lambda_{i}I)a_{i}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Wobei 
\begin_inset Formula $I$
\end_inset

 die 
\begin_inset Formula $n$
\end_inset

-dimensionale Identitätsmatrix darstellt.
 Dies ist aber offenbar ein Eigenwertproblem 
\begin_inset CommandInset citation
LatexCommand cite
key "anton-1998"

\end_inset

.
 Es sind also 
\begin_inset Formula $a_{i}$
\end_inset

 genau die Eigenvektoren von 
\begin_inset Formula $\Sigma$
\end_inset

 und 
\begin_inset Formula $\lambda_{i}$
\end_inset

 die Eigenwerte.
\end_layout

\begin_layout Standard
Nun ist 
\begin_inset Formula $a$
\end_inset

 also genau die Basistransformationsmatrix, die uns einen Vektor gegeben
 zur kanonischen Basis in den Vektorraum zur Basis der Hauptkomponenten
 transformiert.
 Die Hauptkomponenten sind also genau die kanonischen Basisvektoren transformier
t mit 
\begin_inset Formula $a$
\end_inset

, dies sind aber genau die Vektoren 
\begin_inset Formula $a_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
Zwar ist die Berechnung von Eigenvektoren und -werten für höherdimensionale
 Matrizen kein triviales Problem, es gibt aber eine Reihe stabiler und effizient
er Algorithmen zur Berechnung.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
todo{Ref}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Eigenfaces
\end_layout

\begin_layout Standard
Viele frühe Gesichtserkennungsverfahren basieren auf Merkmalen, die menschenvers
tehbar sind, wie beispielsweise Haarfarbe, Augenfarbe oder Position der
 Gesichtsteile zueinander.
 Diese Algorithmen ignorieren, dass solche Merkmale korreliert sein können,
 oder aber zu wenig Information transportieren können 
\begin_inset CommandInset citation
LatexCommand cite
key "turk-1991a"

\end_inset

.
 Zwar scheinen Menschen auf solche Merkmale zur Erkennung von Gesichtern
 zurückzugreifen, da entsprechende Abläufe im Gehirn aber noch nicht vollständig
 verstanden werden, ist es auch möglich das andere Features zur Erkennung
 von Gesichtern wichtiger sind.
 
\end_layout

\begin_layout Standard
Der Eigenface Ansatz abstrahiert von solchen greifbaren Merkmalen.
 Stattdessen wurde ein ein Informationstheoretischer Ansatz gewählt, bei
 dem aus den Gesichtsbildern Merkmale extrahiert werden, die möglichst unkorreli
ert sind 
\begin_inset CommandInset citation
LatexCommand cite
key "turk-1991"

\end_inset

, die sogenannten 
\emph on
Eigenfaces
\emph default
.
 Die Beschreibung der Berechnung von Eigenfaces folgt im wesentlichen 
\begin_inset CommandInset citation
LatexCommand cite
key "turk-1991"

\end_inset

.
\end_layout

\begin_layout Standard
Sei 
\begin_inset Formula $T$
\end_inset

 eine Trainingsmenge der Größe 
\begin_inset Formula $M$
\end_inset

 von Gesichtsbildern, also 
\begin_inset Formula $n\times m$
\end_inset

 Matrizen mit ganzzahligen Werten im Intervall 
\begin_inset Formula $[0,255]$
\end_inset

.
 Diese fassen wir auch als 
\begin_inset Formula $1\times n\cdot m$
\end_inset

-dimensionale Merkmalsvektoren auf.
 Wir suchen jetzt die Matrizen bzw.
 Merkmalsvektoren, aus denen wir mittels Linearkombination alle Trainingsbilder
 rekonstruieren können und die möglichst unkorreliert sind.
 Dies sind aber genau die Hauptkomponenten der Datenmenge.
 Wir suchen also genau die Hauptkomponenten von 
\begin_inset Formula $T$
\end_inset

, dise bilden dann die Eigenfaces.
\end_layout

\begin_layout Standard
Zuerst erstellen wir ein Durchschnittsbild 
\begin_inset Formula $\Psi$
\end_inset

 aller Gesichtsbilder 
\begin_inset Formula $\Gamma_{i}\in T$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\Psi=\frac{1}{M}\sum_{i=1}^{M}\Gamma_{i}\]

\end_inset


\end_layout

\begin_layout Standard
Wir nehmen an, das dies der Erwartungswert eines Gesichtsbildes ist, nehmen
 also an, dass alle Gesichtsbilder normalverteilt um dieses Durchschnittsbild
 sind.
 
\end_layout

\begin_layout Standard
Nun gibt es zu jedem Gesicht 
\begin_inset Formula $\Gamma_{i}$
\end_inset

 ein Differenzbild 
\begin_inset Formula $\Phi_{i}$
\end_inset

, dass die Differenz zu 
\begin_inset Formula $\Psi$
\end_inset

 ist:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\Phi_{i}=\Gamma_{i}-\Psi\]

\end_inset


\end_layout

\begin_layout Standard
Diese geben also den Abstand zum Erwartungswert an und können so genutzt
 werden um eine Kovarianzmatrix aufzustellen:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
Cov(T) & = & \frac{1}{M}\sum_{i=1}^{M}\Phi_{i}\Phi_{i}^{T}\\
 & = & AA^{T}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Wobei 
\begin_inset Formula $A$
\end_inset

 die Matrix 
\begin_inset Formula $\left[\begin{array}{cccc}
\Phi_{1} & \Phi_{2} & \ldots & \Phi_{M}\end{array}\right]$
\end_inset

 ist.
\end_layout

\begin_layout Standard
Von dieser Kovarianzmatrix müssen nun die Eigenvektoren und Eigenwerte berechnet
 werden.
 Wie bereits erwähnt handelt es sich bei 
\begin_inset Formula $\Phi_{i}$
\end_inset

 um 
\begin_inset Formula $n\cdot m$
\end_inset

 große Vektoren.
 Selbst bei niedrig aufgelösten Bildern ergeben sich also sehr große Kovarianzma
trizen.
 Die Berechnung der Eigenvektoren und -werte ist dann aus Speichergründen
 nicht mehr möglich.
\end_layout

\begin_layout Standard
Da wir nur 
\begin_inset Formula $M$
\end_inset

 Trainingsbilder haben, wobei 
\begin_inset Formula $M$
\end_inset

 normalerweise deutlich kleiner ist als die Dimension der Bilder 
\begin_inset Formula $n\cdot m$
\end_inset

, wird es maximal 
\begin_inset Formula $M-1$
\end_inset

 Eigenfaces geben, die aussagekräftig sind.
 Alle weiteren werden mit einem Eigenwert von 
\begin_inset Formula $0$
\end_inset

 assoziiert sein.
 Diese Eigenvektoren können aus den Eigenvektoren der Matrix 
\begin_inset Formula $A^{T}A$
\end_inset

 berechnet werden 
\begin_inset CommandInset citation
LatexCommand cite
key "turk-2005"

\end_inset

.
 Es gilt folgende Abhängigkeit von Eigenvektoren 
\begin_inset Formula $v_{i}$
\end_inset

 und Eigenwerten 
\begin_inset Formula $\lambda_{i}$
\end_inset

 der Kovarianzmatrix 
\begin_inset Formula $C$
\end_inset

 von 
\begin_inset Formula $T$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
Cv_{i}=\lambda_{i}v_{i}\]

\end_inset


\end_layout

\begin_layout Standard
Seien 
\begin_inset Formula $\hat{v}_{i}$
\end_inset

 die Eigenvektoren und 
\begin_inset Formula $\mu_{i}$
\end_inset

 die Eigenwerte der Matrix 
\begin_inset Formula $D=A^{T}A$
\end_inset

, so gilt:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
D\hat{v}_{i} & = & \mu_{i}\hat{v}_{i}\\
A^{T}A\hat{v}_{i} & = & \mu_{i}\hat{v}_{i}\\
AA^{T}A\hat{v}_{i} & = & \mu_{i}A\hat{v}_{i}\\
C\left(A\hat{v}_{i}\right) & = & \mu_{i}\left(A\hat{v}_{i}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Die Eigenvektoren und -werte von 
\begin_inset Formula $C$
\end_inset

 können also berechnet werden als:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
v_{i} & = & \left(A\hat{v}_{i}\right)\\
\lambda_{i} & = & \mu_{i}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Die Matrix 
\begin_inset Formula $A^{T}A$
\end_inset

 ist aber nur eine 
\begin_inset Formula $M\times M$
\end_inset

 Matrix, hat also deutlich weniger Dimensionen als 
\begin_inset Formula $AA^{T}$
\end_inset

.
 Wir können ihre Eigenvektoren und -werte also mit deutlich weniger Speicheraufw
and berechnen und so auch Eigenvektoren von 
\begin_inset Formula $AA^{T}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Wir sortieren nun die Eigenvektoren nach den assoziierten Eigenwerten.
 Diese Hauptkomponenten nennen wir aufgrund ihres gesichtsähnlichen Aussehens
 
\emph on
Eigenfaces 
\emph default
(vgl.
 Abbildung 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Eigenfaces"

\end_inset

).
 Sie spannen einen Unterraum vom Raum aller 
\begin_inset Formula $n\cdot m$
\end_inset

 Bilder auf, den wir 
\emph on
Gesichtsraum
\emph default
 nennen.
 Dieser Gesichtsraum ist maximal 
\begin_inset Formula $M$
\end_inset

-dimensional, die Dimension kann aber auch noch verringert werden, indem
 nur die 
\begin_inset Formula $k<M$
\end_inset

 ersten Hauptkomponenten genutzt werden.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename eigenfaces/eig01.png
	lyxscale 60
	width 25text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename eigenfaces/eig02.png
	lyxscale 60
	width 25text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename eigenfaces/eig03.png
	lyxscale 60
	width 25text%

\end_inset


\family sans

\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Eigenfaces"

\end_inset

Die ersten drei Eigenfaces des AT&T Datenbank Trainingssets.
 Man erkennt deutlich die namensgebende Gesichtsähnlichkeit.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Eigenfaces als Feature-Vektor
\end_layout

\begin_layout Standard
Wie im vorigen Abschnitt beschrieben bilden die Eigenfaces den Gesichtsraum.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "sirovich-1987"

\end_inset

 zeigen Sirovich und Kriby, dass relativ wenig Eigenfaces notwendig sind,
 um Gesichter der Trainingsmenge zu recodieren.
 Man benötigt also im allgemeinen nur 
\begin_inset Formula $M'$
\end_inset

 Eigenfaces.
\end_layout

\begin_layout Standard
Ein Gesicht 
\begin_inset Formula $\Gamma$
\end_inset

 wird mit den Eigenfaces recodiert, indem wir es in den Gesichtsraum projezieren.
 Wir erhalten dann also die Rekodierung 
\begin_inset Formula $\Omega=\left[\begin{array}{cccc}
\omega_{1} & \omega_{2} & \ldots & \omega_{M'}\end{array}\right]^{T}$
\end_inset

, mit
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\omega_{i}=v_{i}(\Gamma-\Psi).\]

\end_inset


\end_layout

\begin_layout Standard
Jedes 
\begin_inset Formula $\omega_{i}$
\end_inset

 ist nun der Anteil des Eigenface 
\begin_inset Formula $i$
\end_inset

 am Gesichtsbild 
\begin_inset Formula $\Gamma$
\end_inset

.
 Wenn wir nun Gesichter erkennen wollen, können wir nun die Recodierung
 
\begin_inset Formula $\Omega$
\end_inset

 als Merkmalsvektor des Gesichtsbild auffassen.
 Diesen nutzen wir als Eingabe für einen Mustererkennungsalgorithmus.
 Eigenfaces selber dienen also nur als Repräsentation oder Features für
 Gesichtsbilder.
 Die eigentliche Erkennung wird nicht mit dem Eigenface Algorithmus ausgeführt.
\end_layout

\begin_layout Subsection
Gesichtserkennungs Algorithmen
\end_layout

\begin_layout Standard
Um nun mit Eigenfaces Gesichter auch tatsächlich zu erkennen können unterschiedl
iche Ansätze gewählt werden.
 Der einfachste Algorithmus ist dabei ein 
\emph on
Nearest Neighbour
\emph default
 Ansatz, von Turk vorgeschlagen 
\begin_inset CommandInset citation
LatexCommand cite
key "turk-1991"

\end_inset

.
 Es wird für jede Person 
\begin_inset Formula $i$
\end_inset

, deren Gesicht erkannt werden soll, ein Durchschnittsbild 
\begin_inset Formula $\Omega_{i}$
\end_inset

 von wenigen Bildern der Person im Gesichtsraum erstellt.
 Nun minimieren wir den quadratischen euklidischen Abstand
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\varepsilon_{k}^{2}=\left\Vert \Omega_{k}-\Omega\right\Vert ^{2}.\]

\end_inset


\end_layout

\begin_layout Standard
Ist 
\begin_inset Formula $\min(\varepsilon_{k})$
\end_inset

 größer als ein Schwellenwert 
\begin_inset Formula $\vartheta$
\end_inset

, so klassifizieren wir es als unbekannt, da es keinem bekannten Gesichtsbild
 nah genug ist.
 Sonst wird es als zur Klasse 
\begin_inset Formula $k$
\end_inset

 zugehörig klassifiziert, zeigt also ein Bild von Person 
\begin_inset Formula $k$
\end_inset

.
 Zusätzlich kann man auch den Abstand des Bildes zum Gesichtsunterraum bestimmen.
 Dieses ist die quadratische euklidische Entfernung des Eingabebildes, zentriert
 um das Durchschnittsbild 
\begin_inset Formula $\Phi=\Gamma-\Psi$
\end_inset

, und seiner Projektion im Gesichtsraum 
\begin_inset Formula $\Phi_{f}=\sum_{i=1}^{M'}v_{i}\omega_{i}$
\end_inset

 (vergleiche Abbildung 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Projektion"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\varepsilon^{2}=\left\Vert \Phi-\Phi_{f}\right\Vert ^{2}\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename eigenfaces/original.png
	width 17text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename eigenfaces/reconstruct.png
	width 17text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename eigenfaces/reconstruct20.png
	width 17text%

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename eigenfaces/rad.JPG
	lyxscale 220
	width 17text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename eigenfaces/reconstruct-rad.png
	width 17text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename eigenfaces/reconstruct-rad20.png
	width 17text%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Projektion"

\end_inset

Ein Gesichtsbild aus der AT&T Datenbank und seine Projektion auf den Gesichtsrau
m mit 165 Eigenfaces und mit 20 Eigenfaces.
 Darunter ein Bild, das kein Gesicht zeigt, projeziert in den Gesichtsraum.
 Man erkennt deutlich das die Projektion des Fahrrads keine Gesichter erzeugt,
 der Abstand zum Gesichtsraum ist zu hoch.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Ist dieser Abstand größer als ein Schwellenwert 
\begin_inset Formula $\vartheta_{f}$
\end_inset

, so kann man davon ausgehen, dass das Bild kein Gesicht darstellt, da es
 sich zu weit entfernt vom Raum der Gesichtsbilder befindet.
\end_layout

\begin_layout Standard
Eine weitere naheliegende Möglichkeit ist die Nutzung eines künstlichen
 neuronalen Netzes, die aber bisher in der Literatur noch nicht vorkommt
\begin_inset Foot
status open

\begin_layout Plain Layout
Es gibt einen Journalbeitrag zu diesem Thema, der aber in großen Teilen
 
\begin_inset CommandInset citation
LatexCommand cite
key "turk-1991"

\end_inset

 plagiiert und daher hier nicht weiter betrachtet wird.
\end_layout

\end_inset

.
 Dabei nutzen wir ein Feed Forward neuronales Netzwerk, wie in 
\begin_inset CommandInset citation
LatexCommand cite
key "Rojas1996"

\end_inset

 beschrieben.
\end_layout

\begin_layout Standard
Als Eingabevektor dient 
\begin_inset Formula $\Omega$
\end_inset

, für jedes bekannte Gesicht gibt es einen Ausgang.
 Das Netzwerk wird dann mit dem Traingsset trainiert, wobei als Ausgabe
 für Gesicht 
\begin_inset Formula $\Gamma_{i}$
\end_inset

 der Zielvektor 
\begin_inset Formula $\left[\begin{array}{cccc}
t_{1} & t_{2} & \ldots & t_{n}\end{array}\right]^{T}$
\end_inset

 mit 
\begin_inset Formula $t_{i}=1$
\end_inset

 und 
\begin_inset Formula $t_{j}=0$
\end_inset

 für alle 
\begin_inset Formula $j\not=i$
\end_inset

 gewählt wird.
 Genutzt wird dafür der Backpropagation Algorithmus, ebenfalls beschrieben
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Rojas1996"

\end_inset

.
\end_layout

\begin_layout Standard
Neue Bilder 
\begin_inset Formula $\Gamma$
\end_inset

 werden dann ebenfalls in den Gesichtsraum projeziert und als Eingabe für
 das neuronale Netz genutzt.
 Der Ausgabevektor 
\begin_inset Formula $\left[\begin{array}{cccc}
a_{1} & a_{2} & \ldots & a_{n}\end{array}\right]^{T}$
\end_inset

 wird nun untersucht, ob es ein 
\begin_inset Formula $a_{i}>\vartheta$
\end_inset

gibt, wobei 
\begin_inset Formula $\vartheta$
\end_inset

 der Schwellenwert für die Zugehörigkeit zu einer Klasse ist.
 Ist dies der Fall, so ist 
\begin_inset Formula $\Gamma$
\end_inset

 ein Bild von Person 
\begin_inset Formula $i$
\end_inset

.
 Ist dies nicht der Fall, so ist das Bild von einer unbekannten Person.
 Zusätzlich kann auch der Abstand zum Gesichtsraum berechnet werden, welcher
 wiederum festlegt, ob das Bild überhaupt ein Gesicht zeigt oder nicht.
\end_layout

\begin_layout Section
Tests des Verfahrens
\end_layout

\begin_layout Standard
Um die vorgestellten Algorithmen zu testen nutzen wir die Gesichtsdatenbank
 der AT&T Laboratories (siehe Abbildung 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Trainingsset"

\end_inset

) in Cambridge 
\begin_inset CommandInset citation
LatexCommand cite
key "samaria-1994"

\end_inset

.
 Darin befinden sich Bilder der Größe 
\begin_inset Formula $112\times92$
\end_inset

 von 40 Personen in unterschiedlichen Posen.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename orl_faces/s4/5.pgm
	width 25text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename orl_faces/s22/6.pgm
	width 25text%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename orl_faces/s34/5.pgm
	width 25text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Trainingsset"

\end_inset

Bilder aus der Trainingsmenge
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Im Cross-Validation Verfahren erstellen wir Trainings- und Testmengen als
 Eingabe für die beiden vorgestellten Gesichtserkennungsalgorithmen 
\begin_inset CommandInset citation
LatexCommand cite
key "webb-2002"

\end_inset

.
 Zusätzlich fügen wir Bilder von Personen in die Testmenge, die nicht in
 der Trainingsmenge enthalten sind.
\end_layout

\begin_layout Subsection
Nearest Neighbour
\end_layout

\begin_layout Standard
Wir wählen zufällig einen Repräsentanten für jede Person in der Trainingsmenge.
 Für jedes Testbild berechnen wir die quadratische euklidische Distanz zu
 allen Repräsentanten.
 Der Repräsentant mit der niedrigsten Distanz ist dann die erkannte person,
 sofern die Distanz kleiner ist als 
\begin_inset Formula $\vartheta$
\end_inset

.
 In Abbildung 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Nearest-Neighbour"

\end_inset

 ist die Erkennungsrate für unterschiedliche 
\begin_inset Formula $\vartheta$
\end_inset

 und Eigenface Anzahlen dargestellt.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
missingfigure{Nearest Neighbour}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Nearest-Neighbour"

\end_inset

Nearest Neighbour
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Künstliches neuronales Netz
\end_layout

\begin_layout Standard
Wir erstellen ein künstliches neuronales Netz.
 Als Basisfunktion für die Eingabe und die Versteckte Schicht wählen wir
 die Sigmoidfunktion.
 Die Ausgabeschicht hat quadratische Fehlerfunktion als Basisfunktion.
 Die versteckte Schicht hat die Größe 
\begin_inset Formula $200$
\end_inset

, die Ausgabeschicht 40, da wir 40 personen erkennen wollen.
 Die Eingabeschicht variiert über die unterschiedlichen Versuche, da wir
 verschieden viele Eigenfaces zur Erkennung nutzen.
 Wir trainieren das Netz mit 100 Zyklen und online Update der Gewichte.
 Als Lernrate wählen wir 
\begin_inset Formula $\eta=0,7$
\end_inset

.
\end_layout

\begin_layout Standard
Zur Erkennung wird nun das neue Gesicht mittels Feed Forward bearbeitet.
 Das Gesicht wird der Klasse zugeordnet, deren Ausgabeneuron die größte
 Ausgabe produziert.
 Sind alle Ausgabeneuronen kleiner als 
\begin_inset Formula $\vartheta$
\end_inset

, so wird das Gesicht als unbekannt eingestuft.
\end_layout

\begin_layout Standard
In Abbildung 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Neuronales netz"

\end_inset

 ist die Erkennungsrate für unterschiedliche 
\begin_inset Formula $\vartheta$
\end_inset

 und Eigenface Anzahlen dargestellt.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
missingfigure{Neuronales netz}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Neuronales netz"

\end_inset

Neuronales netz
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Fazit
\end_layout

\begin_layout Itemize
wie gut funktioniert der Ansatz?
\end_layout

\begin_layout Standard
Dies führt zu der Vorraussetzung, dass alle Bilder unter ähnlichen Bedingungen
 hinsichtlich der Perspektive und des Hintergrundes gemacht werden müssen.
 Verändert sich die Perspektive zum Gesicht deutlich, so kann ein Mittelwert
 von Bildern nicht mehr sinnvoll berechnet werden.
 Die Streuung um diesen wäre nun nicht mehr abhängig von der aufgenommenen
 Person, sondern maßgeblich von der Perspektive.
 Gleiches gilt für den Hintergrund.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "literature"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
